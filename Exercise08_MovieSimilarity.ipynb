{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Taming Big Data With Apache Spark and Python - Hands On!\n",
    "## Exercise 8 - Movie Similarity (Collaborative Filtering)\n",
    "\n",
    "### Setup\n",
    "\n",
    "FindSpark\n",
    "\n",
    "This will circumvent many issues with your system finding spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('c:/users/andy/spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"C:/Users/Andy/Dropbox/FactoryFloor/Repositories/Tutorial_Udemy_SparkPython/Course_Resources/ml-100k/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure your Spark context; master node is local machine\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"MovieSimilarities\")\n",
    "\n",
    "# create a spark context object\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMovieNames():\n",
    "    movieNames = {} # create a dict\n",
    "    file_to_open = data_folder + \"u.ITEM\" #file path\n",
    "    with open(file_to_open, encoding = 'ascii', errors = 'ignore') as f: # open file\n",
    "        for line in f:\n",
    "            fields = line.split('|') # break the lines\n",
    "            movieNames[int(fields[0])] = fields[1] # create key-value\n",
    "    return movieNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDuplicates(userRatings):\n",
    "    ratings = userRatings[1] # the value ((movieID, rating), (movieID, rating))\n",
    "    (movie1, rating1) = ratings[0] \n",
    "    (movie2, rating2) = ratings[1]\n",
    "    return movie1 < movie2 # return only those entries where movieID 2 is greater than movieID 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python 3 doesn't let you pass arond unpacked tuples,\n",
    "# so we explicitly extract the ratings now.\n",
    "def makePairs(userRatings):\n",
    "    ratings = userRatings[1] # the value ((movieID, rating), (movieID, rating))\n",
    "    (movie1, rating1) = ratings[0]\n",
    "    (movie2, rating2) = ratings[1]\n",
    "    return ((movie1, movie2), (rating1, rating2)) #format so its pair of movies and pair of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCosineSimilarity(ratingPairs):\n",
    "    numPairs = 0\n",
    "    sum_xx = sum_yy = sum_xy = 0\n",
    "    for ratingX, ratingY in ratingPairs:\n",
    "        sum_xx += ratingX * ratingX\n",
    "        sum_yy += ratingY * ratingY\n",
    "        sum_xy += ratingX * ratingY\n",
    "        numPairs += 1\n",
    "    \n",
    "    numerator = sum_xy\n",
    "    denominator = sqrt(sum_xx) * sqrt(sum_yy)\n",
    "    \n",
    "    score = 0\n",
    "    if (denominator):\n",
    "        score = (numerator / (float(denominator)))\n",
    "        \n",
    "    return (score, numPairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Program\n",
    "\n",
    "**1) Create a dictionary with movieID and movieNames.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading movie names...\n",
      "Success! Your <class 'dict'> was created with 1682 entries. For instance Clerks (1994) is one of them.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading movie names...\")\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "print(\"Success! Your\", type(nameDict), \"was created with\", len(nameDict), \"entries. For instance\", nameDict.get(42), \"is one of them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Bring in the movie ratings data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your <class 'pyspark.rdd.RDD'> was created with 100000 entries. \n",
      "\n",
      "The first five are:\n",
      " ['196\\t242\\t3\\t881250949', '186\\t302\\t3\\t891717742', '22\\t377\\t1\\t878887116', '244\\t51\\t2\\t880606923', '166\\t346\\t1\\t886397596']\n"
     ]
    }
   ],
   "source": [
    "data = sc.textFile(data_folder + \"u.data\")\n",
    "\n",
    "print(\"Success! Your\", type(data), \"was created with\", data.count(), \"entries. \\\n",
    "\\n\\nThe first five are:\\n\", data.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Key/value stores of dicts: user ID => movie ID, rating.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your <class 'pyspark.rdd.PipelinedRDD'> was created with 100000 entries. \n",
      "\n",
      "The first five are:\n",
      " [(196, (242, 3.0)), (186, (302, 3.0)), (22, (377, 1.0)), (244, (51, 2.0)), (166, (346, 1.0))]\n"
     ]
    }
   ],
   "source": [
    "ratings = data.map(lambda l: l.split()).map(lambda l: (int(l[0]), (int(l[1]), float(l[2]))))\n",
    "\n",
    "print(\"Success! Your\", type(ratings), \"was created with\", ratings.count(), \"entries. \\\n",
    "\\n\\nThe first five are:\\n\", ratings.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Emit every movie rated together by the same user. Self-join to find every combination.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your <class 'pyspark.rdd.PipelinedRDD'> was created with 20200812 entries. \n",
      "\n",
      "The first five are:\n",
      " [(196, ((242, 3.0), (242, 3.0))), (196, ((242, 3.0), (393, 4.0))), (196, ((242, 3.0), (381, 4.0))), (196, ((242, 3.0), (251, 3.0))), (196, ((242, 3.0), (655, 5.0)))]\n"
     ]
    }
   ],
   "source": [
    "joinedRatings = ratings.join(ratings)\n",
    "\n",
    "print(\"Success! Your\", type(joinedRatings), \"was created with\", joinedRatings.count(), \"entries. \\\n",
    "\\n\\nThe first five are:\\n\", joinedRatings.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Filter out duplicate pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your <class 'pyspark.rdd.PipelinedRDD'> was created with 10050406 entries. \n",
      "\n",
      "The filter removed 10150406 entries.\n",
      "\n",
      "The first five are:\n",
      " [(196, ((242, 3.0), (393, 4.0))), (196, ((242, 3.0), (381, 4.0))), (196, ((242, 3.0), (251, 3.0))), (196, ((242, 3.0), (655, 5.0))), (196, ((242, 3.0), (306, 4.0)))]\n"
     ]
    }
   ],
   "source": [
    "uniqueJoinedRatings = joinedRatings.filter(filterDuplicates)\n",
    "\n",
    "print(\"Success! Your\", type(uniqueJoinedRatings), \"was created with\", uniqueJoinedRatings.count(), \"entries. \\\n",
    "\\n\\nThe filter removed\", joinedRatings.count() - uniqueJoinedRatings.count(), \"entries.\\\n",
    "\\n\\nThe first five are:\\n\", uniqueJoinedRatings.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Now key by (movie1, movie2) pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your <class 'pyspark.rdd.PipelinedRDD'> was created with 10050406 entries. \n",
      "\n",
      "The first five are:\n",
      " [((242, 393), (3.0, 4.0)), ((242, 381), (3.0, 4.0)), ((242, 251), (3.0, 3.0)), ((242, 655), (3.0, 5.0)), ((242, 306), (3.0, 4.0))]\n"
     ]
    }
   ],
   "source": [
    "moviePairs = uniqueJoinedRatings.map(makePairs)\n",
    "\n",
    "print(\"Success! Your\", type(moviePairs), \"was created with\", moviePairs.count(), \"entries. \\\n",
    "\\n\\nThe first five are:\\n\", moviePairs.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) Group by key.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your <class 'pyspark.rdd.PipelinedRDD'> was created with 983206 entries. \n",
      "\n",
      "The first five are:\n",
      " [((242, 580), <pyspark.resultiterable.ResultIterable object at 0x000002076C3CD748>), ((242, 692), <pyspark.resultiterable.ResultIterable object at 0x000002076C421128>), ((242, 428), <pyspark.resultiterable.ResultIterable object at 0x000002076C421CF8>), ((242, 340), <pyspark.resultiterable.ResultIterable object at 0x000002076C421400>), ((393, 1241), <pyspark.resultiterable.ResultIterable object at 0x000002076C421710>)]\n"
     ]
    }
   ],
   "source": [
    "moviePairRatings = moviePairs.groupByKey()\n",
    "\n",
    "print(\"Success! Your\", type(moviePairRatings), \"was created with\", moviePairRatings.count(), \"entries. \\\n",
    "\\n\\nThe first five are:\\n\", moviePairRatings.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Compute similarities.**\n",
    "\n",
    "*Save the results if desired\n",
    "moviePairSimilarities.sortByKey()\n",
    "moviePairSimilarities.saveAsTextFile(\"movie-sims\")*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your <class 'pyspark.rdd.PipelinedRDD'> was created with 983206 entries. \n",
      "\n",
      "The first five are:\n",
      " [((242, 580), (0.9443699330874624, 6)), ((242, 692), (0.9203762039948743, 18)), ((242, 428), (0.9419097988977888, 15)), ((242, 340), (0.9455404837184603, 32)), ((393, 1241), (1.0, 1))]\n"
     ]
    }
   ],
   "source": [
    "moviePairSimilarities = moviePairRatings.mapValues(computeCosineSimilarity).cache()\n",
    "\n",
    "print(\"Success! Your\", type(moviePairSimilarities), \"was created with\", moviePairSimilarities.count(), \"entries. \\\n",
    "\\n\\nThe first five are:\\n\", moviePairSimilarities.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract similarities for the movie we care about that are \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Andy\\\\Anaconda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '-f', 'C:\\\\Users\\\\Andy\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-ec43cdaa-d7ba-4395-a03b-e4277274e23f.json']\n"
     ]
    }
   ],
   "source": [
    "print(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-1137dbc4b0e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcoOccurenceThreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmovieID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Filter for movies with this sim that are \"good\" as defined by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '-f'"
     ]
    }
   ],
   "source": [
    "if (len(sys.argv) > 1):\n",
    "    \n",
    "    scoreThreshold = 0.97\n",
    "    coOccurenceThreshold = 50\n",
    "    \n",
    "    movieID = int(sys.argv[1])\n",
    "    \n",
    "    # Filter for movies with this sim that are \"good\" as defined by \n",
    "    # our quality thresholds above\n",
    "    filteredResults = moviePairSimilarities.filter(lambda pairSim: \\\n",
    "                                                  (pairSim[0][0] == movieID or pairSim[0][1] == movieID) \\\n",
    "                                                  and pairSim[1][0] > scoreThreshold and pairsim[1][1] > coOccurenceThreshold)\n",
    "    \n",
    "    # Sort by quality score.\n",
    "    results = filteredResults.map(lambda pairSim: (pairSim[1], pairSim[0])).sortByKey(ascending= False).take(10)\n",
    "    \n",
    "    print(\"Top 10 similar movies for \" + nameDict[movieID])\n",
    "    for result in results:\n",
    "        (sim, pair) = result\n",
    "        # Display the similarity result that isn't the movie we're looking at\n",
    "        similarMovieID = pair[0]\n",
    "        if (similarMovieID == movieID):\n",
    "            similarMovieID == pair[1]\n",
    "        print(nameDict[similarMovieID] + \"\\t score: \" + str(sim[0]) + \"\\t strength: \" + str(sim[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
